syntax = "proto3";

package vllm.openai.v1;

option go_package = "github.com/vllm-project/vllm-grpc/gen/go/openai/v1;openaiv1";
option java_package = "ai.vllm.grpc.openai.v1";
option java_multiple_files = true;

// ================================================================
// vLLM OpenAI-Compatible gRPC Service
// ================================================================

service VLLMService {
  // ===== Health Checks =====
  rpc ServerLive(ServerLiveRequest) returns (ServerLiveResponse) {
    option idempotency_level = NO_SIDE_EFFECTS;
  }

  rpc ServerReady(ServerReadyRequest) returns (ServerReadyResponse) {
    option idempotency_level = NO_SIDE_EFFECTS;
  }

  rpc ModelReady(ModelReadyRequest) returns (ModelReadyResponse) {
    option idempotency_level = NO_SIDE_EFFECTS;
  }

  // ===== Model Information =====
  rpc ListModels(ListModelsRequest) returns (ListModelsResponse) {
    option idempotency_level = NO_SIDE_EFFECTS;
  }

  rpc GetModelInfo(GetModelInfoRequest) returns (ModelInfo) {
    option idempotency_level = NO_SIDE_EFFECTS;
  }

  // ===== Chat Completions =====
  rpc ChatCompletion(ChatCompletionRequest)
      returns (ChatCompletionResponse) {}

  rpc ChatCompletionStream(ChatCompletionRequest)
      returns (stream ChatCompletionChunk) {}

  // ===== Text Completions =====
  rpc Completion(CompletionRequest) returns (CompletionResponse) {}

  rpc CompletionStream(CompletionRequest)
      returns (stream CompletionChunk) {}
}

// ================================================================
// Health Check Messages
// ================================================================

message ServerLiveRequest {}

message ServerLiveResponse {
  bool live = 1;
}

message ServerReadyRequest {}

message ServerReadyResponse {
  bool ready = 1;
}

message ModelReadyRequest {
  string name = 1;
}

message ModelReadyResponse {
  bool ready = 1;
  string name = 2;
}

// ================================================================
// Model Information Messages
// ================================================================

message ListModelsRequest {}

message ListModelsResponse {
  string object = 1;
  repeated ModelInfo data = 2;
}

message GetModelInfoRequest {
  string id = 1;
}

message ModelInfo {
  string id = 1;
  string object = 2;
  int64 created = 3;
  string owned_by = 4;
  int32 max_model_len = 10;
  string dtype = 11;
}

// ================================================================
// Chat Completion Messages
// ================================================================

message ChatCompletionRequest {
  string model = 1;
  repeated ChatMessage messages = 2;

  optional float temperature = 3;
  optional float top_p = 4;
  optional int32 n = 5;
  optional int32 max_tokens = 6;
  repeated string stop = 7;
  optional bool stream = 8;

  optional float presence_penalty = 10;
  optional float frequency_penalty = 11;
  map<int32, float> logit_bias = 12;

  optional string user = 15;

  optional int32 best_of = 20;
  optional bool use_beam_search = 21;
  optional int32 top_k = 22;
  optional float min_p = 23;
  optional float repetition_penalty = 24;
  optional float length_penalty = 25;
  optional bool echo = 26;
  optional int32 seed = 27;
  optional bool logprobs = 28;
  optional int32 top_logprobs = 29;

  optional string request_id = 50;
  optional int32 priority = 51;
}

message ChatMessage {
  string role = 1;
  string content = 2;
  optional string name = 3;

  repeated ToolCall tool_calls = 10;
  optional string tool_call_id = 11;
}

message ToolCall {
  string id = 1;
  string type = 2;
  FunctionCall function = 3;
}

message FunctionCall {
  string name = 1;
  string arguments = 2;
}

message ChatCompletionResponse {
  string id = 1;
  string object = 2;
  int64 created = 3;
  string model = 4;
  repeated ChatChoice choices = 5;
  Usage usage = 6;
  optional string system_fingerprint = 7;
  optional string service_tier = 8;
}

message ChatChoice {
  int32 index = 1;
  ChatMessage message = 2;
  optional ChatLogprobs logprobs = 3;
  string finish_reason = 4;
  optional int32 stop_sequence_index = 5;
}

message ChatLogprobs {
  repeated ChatLogprob content = 1;
}

message ChatLogprob {
  string token = 1;
  double logprob = 2;
  repeated int32 token_bytes = 3;
  repeated TopLogprob top_logprobs = 4;
}

message TopLogprob {
  string token = 1;
  double logprob = 2;
  repeated int32 token_bytes = 3;
}

message ChatCompletionChunk {
  string id = 1;
  string object = 2;
  int64 created = 3;
  string model = 4;
  repeated ChatChunkChoice choices = 5;
  optional Usage usage = 6;
}

message ChatChunkChoice {
  int32 index = 1;
  ChatDelta delta = 2;
  optional ChatLogprobs logprobs = 3;
  optional string finish_reason = 4;
}

message ChatDelta {
  optional string role = 1;
  optional string content = 2;
  repeated ToolCallDelta tool_calls = 3;
  optional string reasoning_content = 4;
}

message ToolCallDelta {
  optional string id = 1;
  optional string type = 2;
  FunctionCallDelta function = 3;
  optional int32 index = 4;
}

message FunctionCallDelta {
  optional string name = 1;
  optional string arguments = 2;
}

// ================================================================
// Text Completion Messages
// ================================================================

message CompletionRequest {
  string model = 1;
  oneof prompt_type {
    string prompt = 2;
    PromptList prompts = 3;
  }

  optional float temperature = 10;
  optional float top_p = 11;
  optional int32 n = 12;
  optional int32 max_tokens = 13;
  repeated string stop = 14;
  optional bool stream = 15;

  optional string suffix = 20;
  optional float presence_penalty = 21;
  optional float frequency_penalty = 22;
  optional int32 best_of = 23;
  map<int32, float> logit_bias = 24;
  optional string user = 25;
  optional bool echo = 26;
  optional bool logprobs = 27;
  optional int32 top_logprobs = 28;

  optional int32 top_k = 40;
  optional float repetition_penalty = 41;
  optional int32 seed = 42;
  optional string request_id = 50;
  optional int32 priority = 51;
}

message PromptList {
  repeated string values = 1;
}

message CompletionResponse {
  string id = 1;
  string object = 2;
  int64 created = 3;
  string model = 4;
  repeated CompletionChoice choices = 5;
  Usage usage = 6;
  optional string system_fingerprint = 7;
}

message CompletionChoice {
  string text = 1;
  int32 index = 2;
  optional CompletionLogprobs logprobs = 3;
  string finish_reason = 4;
  optional string stop_reason = 5;
  repeated int32 token_ids = 6;
  repeated int32 prompt_token_ids = 7;
}

message CompletionLogprobs {
  repeated int32 text_offset = 1;
  repeated double token_logprobs = 2;
  repeated string tokens = 3;
  repeated TopLogprobs top_logprobs = 4;
}

message TopLogprobs {
  map<string, double> top_logprobs = 1;
}

message CompletionChunk {
  string id = 1;
  string object = 2;
  int64 created = 3;
  string model = 4;
  repeated CompletionChunkChoice choices = 5;
  optional Usage usage = 6;
}

message CompletionChunkChoice {
  int32 index = 1;
  string text = 2;
  optional CompletionLogprobs logprobs = 3;
  optional string finish_reason = 4;
  optional string stop_reason = 5;
  repeated int32 token_ids = 6;
  repeated int32 prompt_token_ids = 7;
}

// ================================================================
// Shared Types
// ================================================================

message Usage {
  int32 prompt_tokens = 1;
  int32 completion_tokens = 2;
  int32 total_tokens = 3;
  optional PromptTokensDetails prompt_tokens_details = 4;
}

message PromptTokensDetails {
  optional int32 cached_tokens = 1;
}

